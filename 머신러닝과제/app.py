# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report
from scipy.stats import uniform
import streamlit as st
import warnings
warnings.filterwarnings("ignore")

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train_data = pd.read_csv("ratings_train.txt", sep='\t')
test_data = pd.read_csv("ratings_test.txt", sep='\t')

# 3. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def preprocess(text):
    text = re.sub(r"[^ê°€-í£ã„±-ã…ã…-ã…£\s]", "", str(text))
    return text.strip()

train_data['document'] = train_data['document'].fillna("").apply(preprocess)
test_data['document'] = test_data['document'].fillna("").apply(preprocess)

# 4. ë²¡í„°í™” (TF-IDF)
vectorizer = TfidfVectorizer(max_features=10000)
X_train = vectorizer.fit_transform(train_data['document'])
y_train = train_data['label']
X_test = vectorizer.transform(test_data['document'])
y_test = test_data['label']

# 5. RandomizedSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
param_dist = {
    'C': uniform(0.01, 10),
    'solver': ['liblinear', 'saga']
}

logreg = LogisticRegression(max_iter=1000)
rand_search = RandomizedSearchCV(logreg, param_distributions=param_dist, n_iter=20, 
                                 scoring='accuracy', cv=5, random_state=42, n_jobs=-1)
rand_search.fit(X_train, y_train)

model = rand_search.best_estimator_
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

# 6. Streamlit ì•±
def main():
    st.title("ğŸ¬ ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„")
    menu = st.sidebar.selectbox("ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ë¦¬ë·° ë¶„ì„", "EDA", "ëª¨ë¸ ì •ë³´"])

    if menu == "ë¦¬ë·° ë¶„ì„":
        user_input = st.text_input("ë¦¬ë·°ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
        if user_input:
            processed = preprocess(user_input)
            vectorized = vectorizer.transform([processed])
            prediction = model.predict(vectorized)[0]
            prob = model.predict_proba(vectorized)[0][prediction]
            if prediction == 1:
                st.success(f"ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤! ğŸ˜Š (í™•ë¥ : {prob:.2f})")
            else:
                st.error(f"ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤. ğŸ˜ (í™•ë¥ : {prob:.2f})")

    elif menu == "EDA":
        st.subheader("ë°ì´í„° EDA")

        # í•œê¸€ í°íŠ¸ ì„¤ì •
        import matplotlib.font_manager as fm
        try:
            plt.rcParams['font.family'] = 'Malgun Gothic'
        except:
            try:
                plt.rcParams['font.family'] = 'AppleGothic'
            except:
                st.warning("í•œê¸€ í°íŠ¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        plt.rcParams['axes.unicode_minus'] = False

        st.write("âœ… ë°ì´í„° ê°œìˆ˜:", train_data.shape)
        st.write("âœ… ê²°ì¸¡ì¹˜ ê°œìˆ˜:", train_data.isnull().sum())

        all_words = ' '.join(train_data['document']).split()
        common_words = Counter(all_words).most_common(20)
        st.write("âœ… ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ (ìƒìœ„ 20ê°œ):")
        st.dataframe(pd.DataFrame(common_words, columns=["ë‹¨ì–´", "ë¹ˆë„ìˆ˜"]))

        train_data['length'] = train_data['document'].apply(len)
        fig2, ax2 = plt.subplots()
        ax2.hist(train_data['length'], bins=50, color='skyblue')
        ax2.set_title("ë¦¬ë·° ê¸¸ì´ ë¶„í¬")
        ax2.set_xlabel("ê¸€ì ìˆ˜")
        ax2.set_ylabel("ë¦¬ë·° ê°œìˆ˜")
        st.pyplot(fig2)

    elif menu == "ëª¨ë¸ ì •ë³´":
        st.subheader("ëª¨ë¸ ì„±ëŠ¥ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°")
        st.write("âœ… í…ŒìŠ¤íŠ¸ ì •í™•ë„:", accuracy)
        st.write("âœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:", rand_search.best_params_)
        st.text("ğŸ“‹ ë¶„ë¥˜ ë¦¬í¬íŠ¸:")
        st.text(classification_report(y_test, y_pred, target_names=["ë¶€ì •", "ê¸ì •"]))

if __name__ == "__main__":
    main()
